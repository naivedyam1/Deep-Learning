{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "6Mewk7t59PjC"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BASICS**"
      ],
      "metadata": {
        "id": "FgdUdL3I6s8l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inputs -> Multiply by Werights -> Sum all these Products to create a single number -> Add a bias term -> Pass through an activation function -> Output the number.\n",
        "\n",
        "Mathematically,\n",
        "\n",
        "Output = Activation(Sum of all(Weight[i]*Input[i]) + Bias)"
      ],
      "metadata": {
        "id": "NKWyE2ex62vr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Weights -** They are learnable parameters corresponding to each input. Adjusted in training. If they are -\n",
        "\n",
        "1. Positive -> Input Increases output.\n",
        "\n",
        "2. Negative -> Input Decreases output.\n",
        "\n",
        "3. Small abs value -> Not so important input.\n",
        "\n",
        "4. Large abs value -> Important Input.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Activation Function -** For applying non-linearity."
      ],
      "metadata": {
        "id": "a392BQNr7mWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.arange(1,4,1, dtype = torch.float)\n",
        "\n",
        "weights = torch.randn(3) * 0.1\n",
        "\n",
        "bias = torch.randn(1)\n",
        "\n",
        "weighted_inputs = inputs * weights\n",
        "\n",
        "weighted_sum = weighted_inputs.sum() + bias\n",
        "\n",
        "output = torch.relu(weighted_sum)\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnThNuOX90jz",
        "outputId": "cd538dc0-2f5a-42db-d258-83e9f066b28d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.3807])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 1 - Neuron learning Boolean Logic**"
      ],
      "metadata": {
        "id": "4N_iNF6n_9_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def and_gate(x1: int, x2: int) -> bool:\n",
        "  inputs = torch.tensor([x1, x2], dtype = torch.float)\n",
        "  weights = torch.tensor([1, 1], dtype = torch.float)\n",
        "  bias = -1.5\n",
        "  weighted_sum = torch.dot(inputs, weights) + bias\n",
        "  if weighted_sum > 0:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "def or_gate(x1: int, x2: int) -> int:\n",
        "  inputs = torch.tensor([x1, x2], dtype = torch.float)\n",
        "  weights = torch.ones(2, dtype = torch.float)\n",
        "  bias = -0.5\n",
        "  weighted_sum = torch.dot(inputs, weights) + bias\n",
        "  output = int((1 + torch.sign(weighted_sum))//2)\n",
        "  return output\n",
        "\n",
        "\n",
        "print(or_gate(1,1))\n",
        "print(or_gate(1,0))\n",
        "print(or_gate(0,1))\n",
        "print(or_gate(0,0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCya4r7VAI9s",
        "outputId": "763ebd41-f224-4c0c-e109-e06edc21fae2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "1\n",
            "1\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#nn.Linear does this entire weighted sum.\n",
        "\n",
        "layer = nn.Linear(3,1) #3 input features combined to give 1 output in the layer.\n",
        "\n",
        "x = torch.randn(5,3)\n",
        "\n",
        "z = layer(x)\n",
        "\n",
        "print(z.shape)\n",
        "\n",
        "layer2 = nn.Linear(3, 5) #Same input, but 5 different neurons compute it this time.\n",
        "\n",
        "z2 = layer2(x)\n",
        "\n",
        "print(z2.shape) #5 different outputs for each input - of the 5 neurons give their own prediction for each of the 5 inputs."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQuiQfMxHLVd",
        "outputId": "71199968-1a7f-47ff-be36-6a7b614d15af"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 1])\n",
            "torch.Size([5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Adding Non Linearity to our Neurons**"
      ],
      "metadata": {
        "id": "cokruyYFLBrQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is done by activation functions. Each kind of activation function shows its own distinct behaviour. Without them, no matter how many layers the result will still be linear."
      ],
      "metadata": {
        "id": "0fRBXckKLOc8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Universal Approximation Theorem -** Any neural network with atleast one hidden layer, one non linear activation function and sufficient neurons can approximate any continous function to arbitrary precision."
      ],
      "metadata": {
        "id": "bafIP4GxLjxi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Common Activation Functions**\n",
        "\n",
        "1. ReLU(x) -> Returns max(x,0).\n",
        "\n",
        "2. tanh(x) -> Returns tanh(x).\n",
        "\n",
        "3. Sigmoid(x) -> Returns 1/(1 + exp(-x))"
      ],
      "metadata": {
        "id": "mR6zjIUPM9oK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A Single Neuron -** The following class makes a single neuron after applying all transformations."
      ],
      "metadata": {
        "id": "iSYvJEVTfA8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleNeuron(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(3,1)\n",
        "    self.activation = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    z = self.linear(x)\n",
        "    output = self.activation(z)\n",
        "    return z"
      ],
      "metadata": {
        "id": "wMIcpVlJfUZb"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neuron = SingleNeuron()\n",
        "\n",
        "x = torch.arange(1,4,1, dtype = torch.float)\n",
        "\n",
        "output = neuron(x)\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7z52ljAKgzyy",
        "outputId": "f56cf338-bd9a-4f0e-ed71-93eed9968083"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-1.2742], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This can also be made using nn.Sequential instead of making a custom class.\n"
      ],
      "metadata": {
        "id": "qhI6-jVrhnqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weather = torch.tensor([65.0, 1013.0, 10.0])\n",
        "\n",
        "temp_neuron = nn.Sequential(\n",
        "    nn.Linear(3,1),\n",
        "    nn.ReLU()\n",
        ")\n",
        "\n",
        "prediction = temp_neuron(weather)\n",
        "\n",
        "print(f\"{prediction.item():.1f} F\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eB4wyB1h0zU",
        "outputId": "49fefa25-79d6-4f09-b54f-89b8b19ed652"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0 F\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BUILDING NEURONS**"
      ],
      "metadata": {
        "id": "Fk5RgXssCYB_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Using PyTorch's nn Module**"
      ],
      "metadata": {
        "id": "2-_gHt1hCfAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class neuron(nn.Module):\n",
        "  def __init__(self, num_inputs):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(num_inputs, 1)\n",
        "    self.activation = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    z = self.linear(x)\n",
        "    output = self.activation(z)\n",
        "    return output"
      ],
      "metadata": {
        "id": "U3LATc1qCsIi"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neu = neuron(3)\n",
        "\n",
        "x = torch.arange(1,4, dtype = torch.float)\n",
        "\n",
        "output = neu(x)\n",
        "\n",
        "print(output)\n",
        "\n",
        "print(f\"\\nWeights: {neu.linear.weight}\")\n",
        "print(f\"Bias: {neu.linear.bias}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wRtiFTQGY8Q",
        "outputId": "205ccc77-75f6-4eac-a2d8-1731111721ca"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4538], grad_fn=<SigmoidBackward0>)\n",
            "\n",
            "Weights: Parameter containing:\n",
            "tensor([[-0.3881, -0.4917,  0.5310]], requires_grad=True)\n",
            "Bias: Parameter containing:\n",
            "tensor([-0.4069], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TRAINING**"
      ],
      "metadata": {
        "id": "5IBgVDFydqQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neu = neuron(2)\n",
        "\n",
        "X = torch.tensor([\n",
        "    [1, 1],\n",
        "    [1, 0],\n",
        "    [0, 1],\n",
        "    [0, 0]\n",
        "], dtype = torch.float)\n",
        "\n",
        "y = torch.tensor([[1], [0], [0], [0]], dtype = torch.float)\n",
        "\n",
        "loss = nn.BCELoss()\n",
        "optimizer = optim.SGD(neu.parameters(), lr = 50)\n",
        "\n",
        "for epoch in range(10000):\n",
        "  predictions = neu(X)\n",
        "  loss_value = loss(predictions, y)\n",
        "  optimizer.zero_grad() #clear old gradients\n",
        "  loss_value.backward() #compute new gradients\n",
        "  optimizer.step() #update weights\n",
        "\n",
        "  if epoch % 1000 == 0:\n",
        "    print(f\"Epoch: {epoch}, Loss: {loss_value.item()}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--YMMTUkdt7S",
        "outputId": "41497586-df15-4798-972b-6ff32154dbd4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 0.8739743232727051\n",
            "Epoch: 1000, Loss: 0.0003075942222494632\n",
            "Epoch: 2000, Loss: 0.00016147890710271895\n",
            "Epoch: 3000, Loss: 0.00010947955888696015\n",
            "Epoch: 4000, Loss: 8.281478221761063e-05\n",
            "Epoch: 5000, Loss: 6.659489736193791e-05\n",
            "Epoch: 6000, Loss: 5.5677566706435755e-05\n",
            "Epoch: 7000, Loss: 4.7847854148130864e-05\n",
            "Epoch: 8000, Loss: 4.1961189708672464e-05\n",
            "Epoch: 9000, Loss: 3.734319398063235e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Predictions:\")\n",
        "with torch.no_grad():  #disables gradient tracking\n",
        "    for i, (input_vals, target_val) in enumerate(zip(X, y)):\n",
        "        pred = neu(input_vals.unsqueeze(0))\n",
        "        print(f\"{input_vals.tolist()} → {pred.item():.3f} (target: {target_val.item()})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAZJ4VY90hxf",
        "outputId": "b82665b1-af62-469f-ce5b-72a0792be047"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions:\n",
            "[1.0, 1.0] → 1.000 (target: 1.0)\n",
            "[1.0, 0.0] → 0.000 (target: 0.0)\n",
            "[0.0, 1.0] → 0.000 (target: 0.0)\n",
            "[0.0, 0.0] → 0.000 (target: 0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TRAINING WITH JUST TENSORS**"
      ],
      "metadata": {
        "id": "c51K7mY9vOgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class manualNeuron:\n",
        "  def __init__(self, num_inputs):\n",
        "    self.weights = torch.randn(num_inputs, requires_grad=True) #autograd starts tracking all operations on this tensor because of requires_grad. Used for computing gradients.\n",
        "    self.bias = torch.randn(1, requires_grad = True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    weighted_sum = torch.dot(self.weights, x) + self.bias\n",
        "    output = 1/(1 + torch.exp(-weighted_sum))\n",
        "    return output\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.weights, self.bias]\n"
      ],
      "metadata": {
        "id": "BUj81rZ0vTbn"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neu = manualNeuron(2)\n",
        "\n",
        "for epoch in range(10001):\n",
        "  total_loss = 0\n",
        "  for i in range(len(X)):\n",
        "    prediction = neu.forward(X[i])\n",
        "    loss = -(y[i]*torch.log(prediction/(1-prediction)) + torch.log(1 - prediction))\n",
        "    total_loss += loss.item()\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "      for param in neu.parameters():\n",
        "        param -= 10 * param.grad\n",
        "        param.grad.zero_()\n",
        "\n",
        "  if(epoch % 1000 == 0):\n",
        "    print(f\"Epoch {epoch} - Loss: {total_loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qO0qmo_2zHCS",
        "outputId": "6add68a5-9891-4f67-d3a0-e7837f6fa077"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 - Loss: 14.640341758733484\n",
            "Epoch 1000 - Loss: 0.0007983411778695881\n",
            "Epoch 2000 - Loss: 0.0005402818060247228\n",
            "Epoch 3000 - Loss: 0.00040997528412844986\n",
            "Epoch 4000 - Loss: 0.0003306364014861174\n",
            "Epoch 5000 - Loss: 0.0002770490027614869\n",
            "Epoch 6000 - Loss: 0.0002381254525971599\n",
            "Epoch 7000 - Loss: 0.00020850081637036055\n",
            "Epoch 8000 - Loss: 0.00018555224596639164\n",
            "Epoch 9000 - Loss: 0.00016713384684408084\n",
            "Epoch 10000 - Loss: 0.00015258989878930151\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(X)):\n",
        "  pred = neu.forward(X[i])\n",
        "  print(f\"{X[i][0]} & {X[i][1]} = {pred[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-I7u6x8CENI",
        "outputId": "d1a2930c-d9d3-487e-eee2-03ad75a55d3f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0 & 1.0 = 0.9999371767044067\n",
            "1.0 & 0.0 = 4.4769236410502344e-05\n",
            "0.0 & 1.0 = 4.479776180232875e-05\n",
            "0.0 & 0.0 = 1.2589247407133325e-13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LOSS FUNCTION**"
      ],
      "metadata": {
        "id": "u6t4t-DyGBAQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Properties of a good Loss:**\n",
        "\n",
        "1. Must be non negative.\n",
        "2. Zero at perfection.\n",
        "3. Gradients must exist at every point for gradient descent.\n",
        "4. Monotonic i.e. more error -> more loss.\n",
        "5. Smooth for stable optimization."
      ],
      "metadata": {
        "id": "VmGuitLhGGSp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examples -\n",
        "\n",
        "1. nn.MSELoss()\n",
        "\n",
        "    **Formula -** 1/sample.size() * sum((pred[i] - actual[i])**2)\n",
        "\n",
        "    **Name -** Mean Square Error Loss\n",
        "\n",
        "  Corresponds to Maximum Likelihood under Gaussian Distribution.\n",
        "\n",
        "2. nn.BCELoss()\n",
        "\n",
        "    **Formula -** -1/sample.size() * sum(actual[i]*log(pred[i]) + (1-actual[i])*log(1-pred[i]))\n",
        "\n",
        "    **Name -** Binary Cross Entropy Loss\n",
        "\n",
        "    Corresponds to Maximum Likelihood under Bernoulli Distribution\n",
        "\n",
        "3. nn.CrossEntropyLoss()\n",
        "\n",
        "    **Formula -** -1/sample.size() * sum(sum(actual[i][k]*log(pred[i][k])))\n",
        "\n",
        "    **Name -** Cross Entropy Loss\n",
        "\n",
        "    Corresponds to Negative Log Likelihood for multi class classification problems.\n"
      ],
      "metadata": {
        "id": "QHBQgVPVH2R8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training goal - smaller loss in each iteration. Loss tells us the directions in which we need to adjust weights and thus guides learning.**"
      ],
      "metadata": {
        "id": "C3ecJH0sIh0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Linear(2,1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "X = torch.tensor([[1,2]], dtype = torch.float)\n",
        "y = torch.tensor([[1]], dtype = torch.float)\n",
        "\n",
        "prediction = model(X)\n",
        "\n",
        "mse = nn.MSELoss()\n",
        "bce = nn.BCELoss()\n",
        "ce = nn.CrossEntropyLoss()\n",
        "\n",
        "print(mse(prediction, y))\n",
        "print(bce(prediction, y))\n",
        "print(ce(prediction, y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcf9yfLfoIMC",
        "outputId": "d57eeb52-6247-4631-febe-bc3213b41a11"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.3095, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.8126, grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "tensor(-0., grad_fn=<DivBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CONCEPT OF LEARNING**"
      ],
      "metadata": {
        "id": "f0kstRALp_e3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mathematically, ML is an optimization problem - find parameters that minimize loss. Mathematically,\n",
        "\n",
        "\n",
        "Find Θ* = argmin(L(Θ)) where -\n",
        "\n",
        "\n",
        "Θ = Model Parameters.\n",
        "\n",
        "L(Θ) = Loss Function.\n",
        "\n",
        "Θ* = Optimal Parameters minimizing loss.\n",
        "\n",
        "argmin = Argument that minimizes."
      ],
      "metadata": {
        "id": "jMfZiqjaqLDL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine the Loss as a hilly landscape, start at a random point and take a step down in each iteration until you find the minimal point. The most downward direction is the opposite of gradient so this is called gradient descent. Learning is this entire process where parameters are updating."
      ],
      "metadata": {
        "id": "Cpwom1serKxR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thus, learning process is just do a forward pass -> make predictions -> calculate new loss -> calculate gradients (backward pass) -> move in the direction opposite to gradient aka update weights -> repeat"
      ],
      "metadata": {
        "id": "ZN3xepXhr1Vc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Linear(1,1)\n",
        "\n",
        "X = torch.tensor([[1],[2],[3]], dtype = torch.float)\n",
        "y = torch.tensor([[2],[4],[6]], dtype = torch.float)\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01)\n",
        "\n",
        "for epoch in range(10000):\n",
        "  predictions = model(X)\n",
        "  current_loss = loss(predictions, y)\n",
        "  optimizer.zero_grad()\n",
        "  current_loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  #if epoch%1000 == 0:\n",
        "    #print(f\"Epoch: {epoch}, Loss: {current_loss.item()}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "  #print(model.weight)\n",
        "  #print(model.bias)\n",
        "  print(model(torch.tensor([10], dtype = torch.float)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIjfZTP1KVyo",
        "outputId": "9f5cc308-9837-46f0-bc93-49004fd0efd0"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([20.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learning rate lr is a hyperparameter balancing speed and stability. Bigger eta = faster learning, smaller eta = more stable and lesser oscillation."
      ],
      "metadata": {
        "id": "f6kmR1JroPFg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimal Learning Rate Theorem -** For a quadratic Loss (w^TAw)/2, optimal lr = 2/(lambda_min + lambda_max) where lambda_min, lambda_max are smallest and largest eigenvalues of A."
      ],
      "metadata": {
        "id": "-9CKHwFKojXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usually we don't know A so we start by lr = 0.01 or 0.001, use adaptive optimizers like Adam and RMSProp or use learning rate schedules."
      ],
      "metadata": {
        "id": "jckA6w2-pM-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task -** Implement a neuron that can learn a linear relationship y = mx + c from scratch."
      ],
      "metadata": {
        "id": "HkX7_8eqrs-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Training Data\n",
        "\n",
        "x = torch.tensor([1,2,3,4,5,6], dtype = torch.float)\n",
        "y = torch.tensor([15,25,35,45,55,65], dtype = torch.float)\n",
        "\n",
        "#Starting weights\n",
        "w = torch.randn(1, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "\n",
        "print(w)\n",
        "\n",
        "#Hyperparameter\n",
        "lr = 0.001\n",
        "\n",
        "#Training Loop\n",
        "for epoch in range(1000):\n",
        "  prediction = w*x + b #The model has exactly 1 feature so taking w as a single number is fine. Else take it as a vector with size = input features and do dot product instead.\n",
        "  loss = ((prediction - y)**2).mean()\n",
        "  loss.backward()\n",
        "  with torch.no_grad():\n",
        "    w -= lr*w.grad\n",
        "    b -= lr*b.grad\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "  if epoch % 100 == 0:\n",
        "    print(f\"Epoch: {epoch}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpNmgJZnr5O6",
        "outputId": "3640954b-8c9c-4d01-dd59-28605d0b302a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.0337], requires_grad=True)\n",
            "Epoch: 0, Loss: 1969.27783203125\n",
            "Epoch: 100, Loss: 4.903125286102295\n",
            "Epoch: 200, Loss: 1.8078700304031372\n",
            "Epoch: 300, Loss: 1.6764503717422485\n",
            "Epoch: 400, Loss: 1.5584176778793335\n",
            "Epoch: 500, Loss: 1.4487032890319824\n",
            "Epoch: 600, Loss: 1.346713900566101\n",
            "Epoch: 700, Loss: 1.2519042491912842\n",
            "Epoch: 800, Loss: 1.1637693643569946\n",
            "Epoch: 900, Loss: 1.081840991973877\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(prediction)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hR9Y3sPwB2T",
        "outputId": "5d506bca-f03d-4fa5-e83b-1b0b91e83700"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([13.2472, 23.7814, 34.3156, 44.8498, 55.3840, 65.9182],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([15., 25., 35., 45., 55., 65.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MAKING PREDICTIONS**"
      ],
      "metadata": {
        "id": "OLSy7z17xQoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neuron = nn.Sequential(\n",
        "    nn.Linear(3,1)\n",
        ")\n",
        "\n",
        "x = torch.tensor([\n",
        "    [1,2,3],\n",
        "    [0.5, 1.5, 2.5],\n",
        "    [10, 11, 12],\n",
        "    [101,102,103],\n",
        "    [1.21, 2.21, 3.21]\n",
        "], dtype = torch.float)\n",
        "\n",
        "y = torch.tensor([\n",
        "    [6],\n",
        "    [4.5],\n",
        "    [33],\n",
        "    [306],\n",
        "    [6.63]\n",
        "])\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer = optim.SGD(neuron.parameters(), lr = 0.0001)\n",
        "\n",
        "for epoch in range(1000):\n",
        "  predictions = neuron(x)\n",
        "  loss_val = loss(predictions, y)\n",
        "  optimizer.zero_grad()\n",
        "  loss_val.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(f\"Epoch: {epoch}, Loss: {loss_val.item()}\")\n",
        "\n",
        "print(neuron(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vyyc-jvwxWIF",
        "outputId": "5984ba8e-4f5e-460f-a220-fe2014b2dcbc"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 25256.03125\n",
            "Epoch: 100, Loss: 0.45262080430984497\n",
            "Epoch: 200, Loss: 0.41453108191490173\n",
            "Epoch: 300, Loss: 0.3796462416648865\n",
            "Epoch: 400, Loss: 0.3476983308792114\n",
            "Epoch: 500, Loss: 0.31843769550323486\n",
            "Epoch: 600, Loss: 0.29163965582847595\n",
            "Epoch: 700, Loss: 0.26709598302841187\n",
            "Epoch: 800, Loss: 0.2446199208498001\n",
            "Epoch: 900, Loss: 0.22403264045715332\n",
            "tensor([[  6.5173],\n",
            "        [  5.0203],\n",
            "        [ 33.4636],\n",
            "        [305.9207],\n",
            "        [  7.1461]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class spamNeuron(nn.Module):\n",
        "  def __init__(self, input_num):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(input_num, 1)\n",
        "    self.activation = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    logit = self.linear(x)\n",
        "    prediction = self.activation(logit)\n",
        "    return prediction"
      ],
      "metadata": {
        "id": "t7igLdrk3KlQ"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spam_detector = spamNeuron(100)\n",
        "\n",
        "email = torch.randn(1, 100)\n",
        "\n",
        "prediction = spam_detector(email)\n",
        "\n",
        "if prediction > 0.5:\n",
        "  print(\"Spam\")\n",
        "else:\n",
        "  print(\"Not Spam\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1B9GbA-V4aOk",
        "outputId": "d46f3ec3-8049-4f4b-865e-b4dc6ff04e68"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note -** When making predictions use torch.no_grad() to disable gradient computation. This is because during training, PyTorch computes gradients and stores the entire computation graph in memory. Explictly asking it not to compute it saves this storage."
      ],
      "metadata": {
        "id": "tIgYP07p5Mvs"
      }
    }
  ]
}