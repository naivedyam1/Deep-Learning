{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyORLIgMw7iV/h98PYTxIoG6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim"],"metadata":{"id":"6Mewk7t59PjC","executionInfo":{"status":"ok","timestamp":1768611629251,"user_tz":-330,"elapsed":5852,"user":{"displayName":"Naivedyam Mishra","userId":"06200968372519865393"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["# **BASICS**"],"metadata":{"id":"FgdUdL3I6s8l"}},{"cell_type":"markdown","source":["Inputs -> Multiply by Werights -> Sum all these Products to create a single number -> Add a bias term -> Pass through an activation function -> Output the number.\n","\n","Mathematically,\n","\n","Output = Activation(Sum of all(Weight[i]*Input[i]) + Bias)"],"metadata":{"id":"NKWyE2ex62vr"}},{"cell_type":"markdown","source":["**Weights -** They are learnable parameters corresponding to each input. Adjusted in training. If they are -\n","\n","1. Positive -> Input Increases output.\n","\n","2. Negative -> Input Decreases output.\n","\n","3. Small abs value -> Not so important input.\n","\n","4. Large abs value -> Important Input.\n","\n","\n","\n","\n","**Activation Function -** For applying non-linearity."],"metadata":{"id":"a392BQNr7mWj"}},{"cell_type":"code","source":["inputs = torch.arange(1,4,1, dtype = torch.float)\n","\n","weights = torch.randn(3) * 0.1\n","\n","bias = torch.randn(1)\n","\n","weighted_inputs = inputs * weights\n","\n","weighted_sum = weighted_inputs.sum() + bias\n","\n","output = torch.relu(weighted_sum)\n","\n","print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MnThNuOX90jz","executionInfo":{"status":"ok","timestamp":1768611629403,"user_tz":-330,"elapsed":156,"user":{"displayName":"Naivedyam Mishra","userId":"06200968372519865393"}},"outputId":"1f0a17c4-fb7b-4478-8515-3a4ef0531d41"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1.0998])\n"]}]},{"cell_type":"markdown","source":["**Example 1 - Neuron learning Boolean Logic**"],"metadata":{"id":"4N_iNF6n_9_D"}},{"cell_type":"code","source":["def and_gate(x1: int, x2: int) -> bool:\n","  inputs = torch.tensor([x1, x2], dtype = torch.float)\n","  weights = torch.tensor([1, 1], dtype = torch.float)\n","  bias = -1.5\n","  weighted_sum = torch.dot(inputs, weights) + bias\n","  if weighted_sum > 0:\n","    return True\n","  else:\n","    return False\n","\n","def or_gate(x1: int, x2: int) -> int:\n","  inputs = torch.tensor([x1, x2], dtype = torch.float)\n","  weights = torch.ones(2, dtype = torch.float)\n","  bias = -0.5\n","  weighted_sum = torch.dot(inputs, weights) + bias\n","  output = int((1 + torch.sign(weighted_sum))//2)\n","  return output\n","\n","\n","print(or_gate(1,1))\n","print(or_gate(1,0))\n","print(or_gate(0,1))\n","print(or_gate(0,0))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DCya4r7VAI9s","executionInfo":{"status":"ok","timestamp":1768611629464,"user_tz":-330,"elapsed":60,"user":{"displayName":"Naivedyam Mishra","userId":"06200968372519865393"}},"outputId":"4e3dff2a-e5d5-40b2-816c-d8c51fbdeada"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["1\n","1\n","1\n","0\n"]}]},{"cell_type":"code","source":["#nn.Linear does this entire weighted sum.\n","\n","layer = nn.Linear(3,1) #3 input features combined to give 1 output in the layer.\n","\n","x = torch.randn(5,3)\n","\n","z = layer(x)\n","\n","print(z.shape)\n","\n","layer2 = nn.Linear(3, 5) #Same input, but 5 different neurons compute it this time.\n","\n","z2 = layer2(x)\n","\n","print(z2.shape) #5 different outputs for each input - of the 5 neurons give their own prediction for each of the 5 inputs."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yQuiQfMxHLVd","executionInfo":{"status":"ok","timestamp":1768611920790,"user_tz":-330,"elapsed":45,"user":{"displayName":"Naivedyam Mishra","userId":"06200968372519865393"}},"outputId":"88d68300-c811-474c-8497-422bbd761ec2"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([5, 1])\n","torch.Size([5, 5])\n"]}]},{"cell_type":"markdown","source":["# **Adding Non Linearity to our Neurons**"],"metadata":{"id":"cokruyYFLBrQ"}},{"cell_type":"markdown","source":["This is done by activation functions. Each kind of activation function shows its own distinct behaviour. Without them, no matter how many layers the result will still be linear."],"metadata":{"id":"0fRBXckKLOc8"}},{"cell_type":"markdown","source":["**Universal Approximation Theorem -** Any neural network with atleast one hidden layer, one non linear activation function and sufficient neurons can approximate any continous function to arbitrary precision."],"metadata":{"id":"bafIP4GxLjxi"}},{"cell_type":"markdown","source":["**Common Activation Functions**\n","\n","1. ReLU(x) -> Returns max(x,0).\n","\n","2. tanh(x) -> Returns tanh(x).\n","\n","3. Sigmoid(x) -> Returns 1/(1 + exp(-x))"],"metadata":{"id":"mR6zjIUPM9oK"}},{"cell_type":"markdown","source":["**A Single Neuron -** The following class makes a single neuron after applying all transformations."],"metadata":{"id":"iSYvJEVTfA8-"}},{"cell_type":"code","source":["class SingleNeuron(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.linear = nn.Linear(3,1)\n","    self.activation = nn.ReLU()\n","\n","  def forward(self, x):\n","    z = self.linear(x)\n","    output = self.activation(z)\n","    return z"],"metadata":{"id":"wMIcpVlJfUZb","executionInfo":{"status":"ok","timestamp":1768611629787,"user_tz":-330,"elapsed":2,"user":{"displayName":"Naivedyam Mishra","userId":"06200968372519865393"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["neuron = SingleNeuron()\n","\n","x = torch.arange(1,4,1, dtype = torch.float)\n","\n","output = neuron(x)\n","\n","print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7z52ljAKgzyy","executionInfo":{"status":"ok","timestamp":1768611629796,"user_tz":-330,"elapsed":9,"user":{"displayName":"Naivedyam Mishra","userId":"06200968372519865393"}},"outputId":"4154fde8-80dc-48b3-b9f9-c603a0866cb1"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0.5369], grad_fn=<ViewBackward0>)\n"]}]},{"cell_type":"markdown","source":["This can also be made using nn.Sequential instead of making a custom class.\n"],"metadata":{"id":"qhI6-jVrhnqd"}},{"cell_type":"code","source":["weather = torch.tensor([65.0, 1013.0, 10.0])\n","\n","temp_neuron = nn.Sequential(\n","    nn.Linear(3,1),\n","    nn.ReLU()\n",")\n","\n","prediction = temp_neuron(weather)\n","\n","print(f\"{prediction.item():.1f} F\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_eB4wyB1h0zU","executionInfo":{"status":"ok","timestamp":1768611629796,"user_tz":-330,"elapsed":4,"user":{"displayName":"Naivedyam Mishra","userId":"06200968372519865393"}},"outputId":"a0a4b6b1-6943-4e5a-8fb5-a35f1767b138"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["0.0 F\n"]}]},{"cell_type":"markdown","source":["# **BUILDING NEURONS**"],"metadata":{"id":"Fk5RgXssCYB_"}},{"cell_type":"markdown","source":["**1. Using PyTorch's nn Module**"],"metadata":{"id":"2-_gHt1hCfAM"}},{"cell_type":"code","source":["class neuron(nn.Module):\n","  def __init__(self, num_inputs):\n","    super().__init__()\n","    self.linear = nn.Linear(num_inputs, 1)\n","    self.activation = nn.Sigmoid()\n","\n","  def forward(self, x):\n","    z = self.linear(x)\n","    output = self.activation(z)\n","    return output"],"metadata":{"id":"U3LATc1qCsIi","executionInfo":{"status":"ok","timestamp":1768611629797,"user_tz":-330,"elapsed":1,"user":{"displayName":"Naivedyam Mishra","userId":"06200968372519865393"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["neu = neuron(3)\n","\n","x = torch.arange(1,4, dtype = torch.float)\n","\n","output = neu(x)\n","\n","print(output)\n","\n","print(f\"\\nWeights: {neu.linear.weight}\")\n","print(f\"Bias: {neu.linear.bias}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6wRtiFTQGY8Q","executionInfo":{"status":"ok","timestamp":1768611630016,"user_tz":-330,"elapsed":219,"user":{"displayName":"Naivedyam Mishra","userId":"06200968372519865393"}},"outputId":"972e96e4-6025-4e06-96a9-659564aeba7d"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0.5729], grad_fn=<SigmoidBackward0>)\n","\n","Weights: Parameter containing:\n","tensor([[-0.1170, -0.2441,  0.3263]], requires_grad=True)\n","Bias: Parameter containing:\n","tensor([-0.0800], requires_grad=True)\n"]}]},{"cell_type":"markdown","source":["# **TRAINING**"],"metadata":{"id":"5IBgVDFydqQn"}},{"cell_type":"code","source":["neu = neuron(2)\n","\n","X = torch.tensor([\n","    [1, 1],\n","    [1, 0],\n","    [0, 1],\n","    [0, 0]\n","], dtype = torch.float)\n","\n","y = torch.tensor([[1], [0], [0], [0]], dtype = torch.float)\n","\n","loss = nn.BCELoss()\n","optimizer = optim.SGD(neu.parameters(), lr = 50)\n","\n","for epoch in range(10000):\n","  predictions = neu(X)\n","  loss_value = loss(predictions, y)\n","  optimizer.zero_grad()\n","  loss_value.backward()\n","  optimizer.step()\n","\n","  if epoch % 1000 == 0:\n","    print(f\"Epoch: {epoch}, Loss: {loss_value.item()}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"--YMMTUkdt7S","executionInfo":{"status":"ok","timestamp":1768611643280,"user_tz":-330,"elapsed":13261,"user":{"displayName":"Naivedyam Mishra","userId":"06200968372519865393"}},"outputId":"6779a950-4931-4d0d-9643-727f5d3f4100"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0, Loss: 0.6041615009307861\n","Epoch: 1000, Loss: 0.0002849958837032318\n","Epoch: 2000, Loss: 0.0001550256129121408\n","Epoch: 3000, Loss: 0.0001064603275153786\n","Epoch: 4000, Loss: 8.108050678856671e-05\n","Epoch: 5000, Loss: 6.54860123177059e-05\n","Epoch: 6000, Loss: 5.491523916134611e-05\n","Epoch: 7000, Loss: 4.726626502815634e-05\n","Epoch: 8000, Loss: 4.148826701566577e-05\n","Epoch: 9000, Loss: 3.698470754898153e-05\n"]}]},{"cell_type":"code","source":["print(\"Predictions:\")\n","with torch.no_grad():\n","    for i, (input_vals, target_val) in enumerate(zip(X, y)):\n","        pred = neu(input_vals.unsqueeze(0))\n","        print(f\"{input_vals.tolist()} → {pred.item():.3f} (target: {target_val.item()})\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RAZJ4VY90hxf","executionInfo":{"status":"ok","timestamp":1768611643290,"user_tz":-330,"elapsed":8,"user":{"displayName":"Naivedyam Mishra","userId":"06200968372519865393"}},"outputId":"452419a5-db12-4387-a05c-9c768ff6bde1"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Predictions:\n","[1.0, 1.0] → 1.000 (target: 1.0)\n","[1.0, 0.0] → 0.000 (target: 0.0)\n","[0.0, 1.0] → 0.000 (target: 0.0)\n","[0.0, 0.0] → 0.000 (target: 0.0)\n"]}]},{"cell_type":"markdown","source":["# **TRAINING WITH JUST TENSORS**"],"metadata":{"id":"c51K7mY9vOgU"}},{"cell_type":"code","source":["class manualNeuron:\n","  def __init__(self, num_inputs):\n","    self.weights = torch.randn(num_inputs, requires_grad=True) #autograd starts tracking all operations on this tensor because of requires_grad. Used for computing gradients.\n","    self.bias = torch.randn(1, requires_grad = True)\n","\n","  def forward(self, x):\n","    weighted_sum = torch.dot(self.weights, x) + self.bias\n","    output = 1/(1 + torch.exp(-weighted_sum))\n","    return output\n","\n","  def parameters(self):\n","    return [self.weights, self.bias]\n"],"metadata":{"id":"BUj81rZ0vTbn","executionInfo":{"status":"ok","timestamp":1768611643294,"user_tz":-330,"elapsed":2,"user":{"displayName":"Naivedyam Mishra","userId":"06200968372519865393"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["neu = manualNeuron(2)\n","\n","for epoch in range(10001):\n","  total_loss = 0\n","  for i in range(len(X)):\n","    prediction = neu.forward(X[i])\n","    loss = -(y[i]*torch.log(prediction/(1-prediction)) + torch.log(1 - prediction))\n","    total_loss += loss.item()\n","    loss.backward()\n","    with torch.no_grad():\n","      for param in neu.parameters():\n","        param -= 10 * param.grad\n","        param.grad.zero_()\n","\n","  if(epoch % 1000 == 0):\n","    print(f\"Epoch {epoch} - Loss: {total_loss}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qO0qmo_2zHCS","executionInfo":{"status":"ok","timestamp":1768611659546,"user_tz":-330,"elapsed":16251,"user":{"displayName":"Naivedyam Mishra","userId":"06200968372519865393"}},"outputId":"eeeb415f-5170-4bd1-f695-82a5b4f4414a"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0 - Loss: 4.81676295195939\n","Epoch 1000 - Loss: 0.0004359752438176656\n","Epoch 2000 - Loss: 0.00025851294049061835\n","Epoch 3000 - Loss: 0.0002047460038738791\n","Epoch 4000 - Loss: 0.00017732665946823545\n","Epoch 5000 - Loss: 0.0001592062326380983\n","Epoch 6000 - Loss: 0.00014430467490456067\n","Epoch 7000 - Loss: 0.00013280069470056333\n","Epoch 8000 - Loss: 0.0001232637332577724\n","Epoch 9000 - Loss: 0.00011539573824848048\n","Epoch 10000 - Loss: 0.00010734897659858689\n"]}]},{"cell_type":"code","source":["for i in range(len(X)):\n","  pred = neu.forward(X[i])\n","  print(f\"{X[i][0]} & {X[i][1]} = {pred[0]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A-I7u6x8CENI","executionInfo":{"status":"ok","timestamp":1768611659564,"user_tz":-330,"elapsed":15,"user":{"displayName":"Naivedyam Mishra","userId":"06200968372519865393"}},"outputId":"f6ae3f06-a4d4-42ea-86a8-e8af75a84926"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["1.0 & 1.0 = 0.9999555349349976\n","1.0 & 0.0 = 3.1009276426630095e-05\n","0.0 & 1.0 = 3.23840431519784e-05\n","0.0 & 0.0 = 4.460907825857424e-14\n"]}]},{"cell_type":"markdown","source":["# **LOSS FUNCTION**"],"metadata":{"id":"u6t4t-DyGBAQ"}},{"cell_type":"markdown","source":["**Properties of a good Loss:**\n","\n","1. Must be non negative.\n","2. Zero at perfection.\n","3. Gradients must exist at every point for gradient descent.\n","4. Monotonic i.e. more error -> more loss.\n","5. Smooth for stable optimization."],"metadata":{"id":"VmGuitLhGGSp"}},{"cell_type":"markdown","source":["Examples - nn.MSELoss(), nn.BCELoss(), nn.CELoss() (cross entropy loss for multi class classification/Softmax Regression)"],"metadata":{"id":"QHBQgVPVH2R8"}},{"cell_type":"markdown","source":["Training goal - smaller loss in each iteration"],"metadata":{"id":"C3ecJH0sIh0w"}}]}