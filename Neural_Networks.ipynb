{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "A netwok has 3 components - an input layer, one or more hidden layers and one output layer. Information travels from left to right."
      ],
      "metadata": {
        "id": "HjSspXgKe-bO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input layer is not composed of neurons. It just contains raw data and number of nodes = number of features of the data."
      ],
      "metadata": {
        "id": "AaNJZscJfY-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hidden layers are the brain of a neural network. Width of a hidden layer is number of neurons in it and depth is the number of layers. Each neuron in a hidden layer is connected to all outputs of the previous layer. They detect complex patterns like layer 1 may detect shapes, layer 2 may combine shapes to make objects, etc."
      ],
      "metadata": {
        "id": "uSdN6xlEf18E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output layer contains number of neurons = number of outputs we want. Activation function used in this is task specific."
      ],
      "metadata": {
        "id": "3TqHA6BzhDY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NEURON TO LAYER**"
      ],
      "metadata": {
        "id": "leOVh8L3qDi4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Say we have n neurons each having m weights. We can then make a weight matrix W of shape m x n where each column vector of the matrix corresponds to the weight vector of that neuron. Similarly we can have a bias vector of size n where each element represents bias of that neuron."
      ],
      "metadata": {
        "id": "J0gLlgYjqOwa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a single neuron, linear step was wTx where w was weight vector. For this operation it is the matrix vector operation x @ W + b and result is a vector containing pre-activation of all neurons."
      ],
      "metadata": {
        "id": "ek-yz8JHq2qr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Layer(nn.Module):\n",
        "  def __init__(self, n_input, n_neurons, activation = None):\n",
        "    super().__init__()\n",
        "    self.weights = nn.Parameter(torch.randn(n_input, n_neurons))\n",
        "    self.bias = nn.Parameter(torch.zeros(n_neurons))\n",
        "    self.activation = activation\n",
        "\n",
        "  def forward_pass(self, x):\n",
        "    logits = x @ self.weights + self.bias\n",
        "    if self.activation is not None:\n",
        "      return self.activation(logits)\n",
        "    else:\n",
        "      return logits\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "DdsxlD7kP91f"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_neurons = 3\n",
        "n_inputs = 5\n",
        "batch_size = 2\n",
        "\n",
        "my_layer = Layer(n_inputs, n_neurons, activation = F.relu)\n",
        "x = torch.randn(batch_size, n_inputs)\n",
        "output = my_layer.forward_pass(x)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmiTmxG-ScJ8",
        "outputId": "089a368b-1bd4-41fd-c115-e5a5da9a3ff7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3])\n"
          ]
        }
      ]
    }
  ]
}