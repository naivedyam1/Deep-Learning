{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "A netwok has 3 components - an input layer, one or more hidden layers and one output layer. Information travels from left to right."
      ],
      "metadata": {
        "id": "HjSspXgKe-bO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input layer is not composed of neurons. It just contains raw data and number of nodes = number of features of the data."
      ],
      "metadata": {
        "id": "AaNJZscJfY-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hidden layers are the brain of a neural network. Width of a hidden layer is number of neurons in it and depth is the number of layers. Each neuron in a hidden layer is connected to all outputs of the previous layer. They detect complex patterns like layer 1 may detect shapes, layer 2 may combine shapes to make objects, etc."
      ],
      "metadata": {
        "id": "uSdN6xlEf18E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output layer contains number of neurons = number of outputs we want. Activation function used in this is task specific."
      ],
      "metadata": {
        "id": "3TqHA6BzhDY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NEURON TO LAYER**"
      ],
      "metadata": {
        "id": "leOVh8L3qDi4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Say we have n neurons each having m weights. We can then make a weight matrix W of shape m x n where each column vector of the matrix corresponds to the weight vector of that neuron. Similarly we can have a bias vector of size n where each element represents bias of that neuron."
      ],
      "metadata": {
        "id": "J0gLlgYjqOwa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a single neuron, linear step was wTx where w was weight vector. For this operation it is the matrix vector operation x @ W + b and result is a vector containing pre-activation of all neurons."
      ],
      "metadata": {
        "id": "ek-yz8JHq2qr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Layer(nn.Module):\n",
        "  def __init__(self, n_input, n_neurons, activation = None):\n",
        "    super().__init__()\n",
        "    self.weights = nn.Parameter(torch.randn(n_input, n_neurons)) #Use nn.Parameter(torch.randn()) instead of torch.randn() directly because nn.Parameter tells PyTorch it is a learnable parameter. Only registered parameters are updated by torch.optim.\n",
        "    self.bias = nn.Parameter(torch.zeros(n_neurons)) #Without nn.Parameters, gradients can still be track upon setting requires_grad = True but we need to pass it in optim everytime.\n",
        "    self.activation = activation\n",
        "\n",
        "  def forward_pass(self, x):\n",
        "    logits = x @ self.weights + self.bias\n",
        "    if self.activation is not None:\n",
        "      return self.activation(logits)\n",
        "    else:\n",
        "      return logits\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "DdsxlD7kP91f"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_neurons = 3\n",
        "n_inputs = 5\n",
        "batch_size = 2\n",
        "\n",
        "my_layer = Layer(n_inputs, n_neurons, activation = F.relu)\n",
        "x = torch.randn(batch_size, n_inputs)\n",
        "output = my_layer.forward_pass(x)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmiTmxG-ScJ8",
        "outputId": "aa97b069-f415-4d08-a493-335181758571"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.5587, 4.1114, 5.3775],\n",
            "        [1.3606, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practice 1 -** Create an output layer for binary classification problem that should take 16 inputs from a previous hidden layer, using sigmoid as the activation function."
      ],
      "metadata": {
        "id": "E0mD5aMUUM1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_inputs = 16\n",
        "n_neurons = 1 #because output is either yes or no. No other features.\n",
        "\n",
        "my_layer = Layer(n_inputs, n_neurons, activation = F.sigmoid)\n",
        "x = torch.randn(n_inputs)\n",
        "output = my_layer.forward_pass(x)\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9l7DgEWWhzt",
        "outputId": "7aeb30c1-bab1-4cd0-d100-845655a7d740"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4462], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practice 2 -** Create a Layer instance that would be suitable as the output layer for a regression problem (like predicting a house price). It should take 8 inputs."
      ],
      "metadata": {
        "id": "UKLxIafBXYI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_inputs = 8\n",
        "n_neurons = 1 #We are just interested in the price so one output - that single number.\n",
        "batch_size = 5\n",
        "\n",
        "my_layer = Layer(n_inputs, n_neurons) #No activation as the problem is generally Linear Regression. No need for activation as we don't need to deal with boundedness.\n",
        "\n",
        "x = torch.randn(batch_size, n_inputs)\n",
        "\n",
        "output = my_layer.forward_pass(x)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFu4YAbsXmbI",
        "outputId": "0c167b2f-3b34-4eb0-f9bc-c2b4488bdb57"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.4546],\n",
            "        [-1.0087],\n",
            "        [-2.7327],\n",
            "        [ 1.9517],\n",
            "        [ 1.0942]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PyTorch built-in Layer Class -** nn.Linear does the same thing as our defined layer."
      ],
      "metadata": {
        "id": "CIAJH1TKZzx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer1 = nn.Linear(in_features=8, out_features=1)\n",
        "x = torch.randn(batch_size, n_inputs)\n",
        "output = layer1(x)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F73a1X3naEXl",
        "outputId": "acb9d3b8-d553-44c4-a753-4a151bfa17ce"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.6360],\n",
            "        [-1.1589],\n",
            "        [-0.9103],\n",
            "        [-0.1377],\n",
            "        [-1.2064]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    }
  ]
}